{
 "metadata": {
  "name": "",
  "signature": "sha256:006bf05862330ed7c866148be3f846e40b28a2ee1b116f0e74b26eca74885cc8"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Time-Series Prediction: Kaggle Bike Sharing Demand"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "@satishkt @sialan\n",
      "\n",
      "Lorem ipsum delorum..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This tells matplotlib not to try opening a new window for each plot.\n",
      "%matplotlib inline\n",
      "\n",
      "import math\n",
      "import random\n",
      "from datetime import datetime, date, time\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import statsmodels.api as sm\n",
      "\n",
      "from sklearn import cross_validation\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "from sklearn import linear_model\n",
      "from sklearn.neighbors import KNeighborsRegressor\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "\n",
      "import seaborn as sns\n",
      "import matplotlib as mpl\n",
      "import matplotlib.pyplot as plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load the data, which is included in sklearn.\n",
      "bike_sharing_demand = pd.read_csv('./data/input/train.csv')\n",
      "prediction_data = pd.read_csv('./data/input/test.csv')\n",
      "             \n",
      "train_data, train_labels = bike_sharing_demand.ix[:, 'datetime':'windspeed'], bike_sharing_demand.ix[:, 'casual':]\n",
      "prediction_data = prediction_data.ix[:, 'datetime':'windspeed']\n",
      "\n",
      "print train_data.columns\n",
      "print train_data.dtypes\n",
      "print train_labels.columns\n",
      "print train_labels.dtypes"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Index([u'datetime', u'season', u'holiday', u'workingday', u'weather', u'temp', u'atemp', u'humidity', u'windspeed'], dtype='object')\n",
        "datetime       object\n",
        "season          int64\n",
        "holiday         int64\n",
        "workingday      int64\n",
        "weather         int64\n",
        "temp          float64\n",
        "atemp         float64\n",
        "humidity        int64\n",
        "windspeed     float64\n",
        "dtype: object\n",
        "Index([u'casual', u'registered', u'count'], dtype='object')\n",
        "casual        int64\n",
        "registered    int64\n",
        "count         int64\n",
        "dtype: object\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Data Exploration"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's get a better handle on what we're working with..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO: Data Exploration"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In order to get the value of the information in the timestamp, we need to extract it. We'll cover some more advanced feature engineering later on but this should help us out of the gate."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's extract the information\n",
      "for dataset in (train_data, prediction_data):\n",
      "    dataset['hour'] = dataset['datetime'].map(lambda x: (datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\")).hour)\n",
      "    dataset['weekday'] = dataset['datetime'].map(lambda x: (datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\")).weekday())\n",
      "    dataset['month'] = dataset['datetime'].map(lambda x: (datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\")).month)\n",
      "\n",
      "prediction_data.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>datetime</th>\n",
        "      <th>season</th>\n",
        "      <th>holiday</th>\n",
        "      <th>workingday</th>\n",
        "      <th>weather</th>\n",
        "      <th>temp</th>\n",
        "      <th>atemp</th>\n",
        "      <th>humidity</th>\n",
        "      <th>windspeed</th>\n",
        "      <th>hour</th>\n",
        "      <th>weekday</th>\n",
        "      <th>month</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 2011-01-20 00:00:00</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "      <td> 1</td>\n",
        "      <td> 10.66</td>\n",
        "      <td> 11.365</td>\n",
        "      <td> 56</td>\n",
        "      <td> 26.0027</td>\n",
        "      <td> 0</td>\n",
        "      <td> 3</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 2011-01-20 01:00:00</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "      <td> 1</td>\n",
        "      <td> 10.66</td>\n",
        "      <td> 13.635</td>\n",
        "      <td> 56</td>\n",
        "      <td>  0.0000</td>\n",
        "      <td> 1</td>\n",
        "      <td> 3</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 2011-01-20 02:00:00</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "      <td> 1</td>\n",
        "      <td> 10.66</td>\n",
        "      <td> 13.635</td>\n",
        "      <td> 56</td>\n",
        "      <td>  0.0000</td>\n",
        "      <td> 2</td>\n",
        "      <td> 3</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 2011-01-20 03:00:00</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "      <td> 1</td>\n",
        "      <td> 10.66</td>\n",
        "      <td> 12.880</td>\n",
        "      <td> 56</td>\n",
        "      <td> 11.0014</td>\n",
        "      <td> 3</td>\n",
        "      <td> 3</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 2011-01-20 04:00:00</td>\n",
        "      <td> 1</td>\n",
        "      <td> 0</td>\n",
        "      <td> 1</td>\n",
        "      <td> 1</td>\n",
        "      <td> 10.66</td>\n",
        "      <td> 12.880</td>\n",
        "      <td> 56</td>\n",
        "      <td> 11.0014</td>\n",
        "      <td> 4</td>\n",
        "      <td> 3</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "              datetime  season  holiday  workingday  weather   temp   atemp  \\\n",
        "0  2011-01-20 00:00:00       1        0           1        1  10.66  11.365   \n",
        "1  2011-01-20 01:00:00       1        0           1        1  10.66  13.635   \n",
        "2  2011-01-20 02:00:00       1        0           1        1  10.66  13.635   \n",
        "3  2011-01-20 03:00:00       1        0           1        1  10.66  12.880   \n",
        "4  2011-01-20 04:00:00       1        0           1        1  10.66  12.880   \n",
        "\n",
        "   humidity  windspeed  hour  weekday  month  \n",
        "0        56    26.0027     0        3      1  \n",
        "1        56     0.0000     1        3      1  \n",
        "2        56     0.0000     2        3      1  \n",
        "3        56    11.0014     3        3      1  \n",
        "4        56    11.0014     4        3      1  "
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Evaluation Metrics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Before we move onto modelling, we can define a few functions to use going forward..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Define the Root-Mean-Squared-Log Error function for scoring predictions\n",
      "def rmsle(actual_values, predicted_values):\n",
      "    squared_log_errors = (np.log(np.array(predicted_values) + 1) - np.log(np.array(actual_values) + 1)) ** 2\n",
      "    mean_squared_errors = np.nansum(squared_log_errors) / len(squared_log_errors)\n",
      "    return np.sqrt(mean_squared_errors)\n",
      "\n",
      "# Define cross-validation loop for training and testing\n",
      "rs = cross_validation.ShuffleSplit(train_data.shape[0], n_iter=10, random_state=0)\n",
      "for train_index, test_index in rs:\n",
      "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('TRAIN:', array([ 4677,  4448,  5705, ...,  9845, 10799,  2732]), 'TEST:', array([6638, 7975, 5915, ..., 6860, 9596, 2663]))\n",
        "('TRAIN:', array([10386,  7496,  9182, ...,  1139,  2328,  2219]), 'TEST:', array([6395, 6832, 9781, ..., 2491, 6925, 6305]))\n",
        "('TRAIN:', array([ 1287,  6686,  3958, ...,  4151,   665, 10420]), 'TEST:', array([ 2075,  2707,  6480, ..., 10843,  7669,  2231]))\n",
        "('TRAIN:', array([5623, 5232, 5102, ..., 9615,  495, 1377]), 'TEST:', array([4024, 5941,   16, ..., 6469,  573, 3649]))\n",
        "('TRAIN:', array([ 121, 2774, 6844, ..., 9362, 6073, 8646]), 'TEST:', array([5481, 8121, 7550, ..., 5653, 6203, 3524]))\n",
        "('TRAIN:', array([3211, 3162, 1709, ...,  459, 1169, 3559]), 'TEST:', array([7341, 6238, 9430, ..., 9801, 5258, 5785]))\n",
        "('TRAIN:', array([ 2695,  8909,  7979, ...,  3635, 10203,  2596]), 'TEST:', array([7930, 1021, 4776, ..., 9878, 8776, 1113]))\n",
        "('TRAIN:', array([3614, 5140,  928, ...,  839, 1852, 1606]), 'TEST:', array([6553, 9310, 1568, ..., 6010,  439, 2633]))\n",
        "('TRAIN:', array([4720, 1323, 4715, ..., 2609, 9758, 9604]), 'TEST:', array([10826,  8944,  5650, ...,  4131,  4755,  2856]))\n",
        "('TRAIN:', array([5584, 6556,  450, ..., 5275,  210,  456]), 'TEST:', array([7766, 3685, 7400, ..., 9496, 2389, 7535]))\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "A Baseline Model (Linear Regression)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Yay."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create linear regression object\n",
      "regr = linear_model.LinearRegression()\n",
      "\n",
      "# Train the model using the training sets\n",
      "for i, (train_index, test_index) in enumerate(rs):\n",
      "    regr.fit(train_data.ix[train_index, 'season':], train_labels.ix[train_index, 'count'])\n",
      "    prediction_values = regr.predict(train_data.ix[test_index, 'season':])\n",
      "    print(\"Cross-Validated Error Loop {0}: {1}\".format(i, rmsle(train_labels.ix[test_index, 'count'], prediction_values)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Cross-Validated Error Loop 0: 1.19646678911\n",
        "Cross-Validated Error Loop 1: 1.16641417964\n",
        "Cross-Validated Error Loop 2: 1.19882096459\n",
        "Cross-Validated Error Loop 3: 1.17903117131\n",
        "Cross-Validated Error Loop 4: 1.12695754455"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Cross-Validated Error Loop 5: 1.16772459075\n",
        "Cross-Validated Error Loop 6: 1.14603545434\n",
        "Cross-Validated Error Loop 7: 1.18627349771\n",
        "Cross-Validated Error Loop 8: 1.17217954141"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Cross-Validated Error Loop 9: 1.14847468724\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "A Classical Statistical Forecasting Approach"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Traditional approaches to time-series forecasting assume an underlying stochastic process with some degree of stationarity. Using this assumption, auto-regressive (AR) approaches using VAR, ARIMA, or GARCH models are very common. However, unable to capture non-linear feature interaction...ADD MORE"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's try some traditional statistical forecasting models, ARIMA\n",
      "# arma = sm.tsa.ARIMA(src_data_model, order=(1,1,1), freq='W').fit(full_output=False, disp=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "A Machine Learning Approach To Time-Series Forecasting"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Modern approaches to time-series forecasting leverage chaos theory and the assumption that the underlying process is not random, but ultimately deterministic and highly non-linear...ADD MORE"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "K Nearest Neighbors"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A non-parametric approach based on ________ paper...ADD MORE"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create knn regression object\n",
      "neigh = KNeighborsRegressor(n_neighbors=5)\n",
      "\n",
      "# Train the model using the training sets\n",
      "for i, (train_index, test_index) in enumerate(rs):\n",
      "    neigh.fit(train_data.ix[train_index, 'season':], train_labels.ix[train_index, 'count'])\n",
      "    prediction_values = neigh.predict(train_data.ix[test_index, 'season':])\n",
      "    print(\"Cross-Validated Error Loop {0}: {1}\".format(i, rmsle(train_labels.ix[test_index, 'count'], prediction_values)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Cross-Validated Error Loop 0: 0.874831380405\n",
        "Cross-Validated Error Loop 1: 0.816448989085"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Cross-Validated Error Loop 2: 0.845769028824"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Cross-Validated Error Loop 3: 0.863034482382"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Cross-Validated Error Loop 4: 0.824645728344"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Cross-Validated Error Loop 5: 0.831427824698"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Cross-Validated Error Loop 6: 0.867591377324"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Cross-Validated Error Loop 7: 0.836196160394"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Cross-Validated Error Loop 8: 0.843380833742"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Cross-Validated Error Loop 9: 0.841398341874"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Support Vector Machines"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A kernel method based on ____ paper...ADD MORE"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create svr objects\n",
      "svr_lin = SVR(kernel='linear', C=1e3)\n",
      "svr_rbf = SVR(kernel='rbf', C=1e3, gamma=0.1)\n",
      "svr_poly = SVR(kernel='poly', C=1e3, degree=2)\n",
      "\n",
      "\"\"\"\n",
      "print(\"For the SVR using a linear kernel:\")\n",
      "for i, (train_index, test_index) in enumerate(rs):\n",
      "    svr_lin.fit(train_data.ix[train_index, 'season':], train_labels.ix[train_index, 'count'])\n",
      "    prediction_values = svr_lin.predict(train_data.ix[test_index, 'season':])\n",
      "    print(\"Cross-Validated Error Loop {0}: {1}\".format(i, rmsle(train_labels.ix[test_index, 'count'], prediction_values)))\n",
      "    \n",
      "# Train the model using the training sets\n",
      "print(\"For the SVR using an rbf kernel:\")\n",
      "for i, (train_index, test_index) in enumerate(rs):\n",
      "    svr_rbf.fit(train_data.ix[train_index, 'season':], train_labels.ix[train_index, 'count'])\n",
      "    prediction_values = svr_rbf.predict(train_data.ix[test_index, 'season':])\n",
      "    print(\"Cross-Validated Error Loop {0}: {1}\".format(i, rmsle(train_labels.ix[test_index, 'count'], prediction_values)))\n",
      "    \n",
      "# Train the model using the training sets\n",
      "print(\"For the SVR using a polynomial kernel or degree 2:\")\n",
      "for i, (train_index, test_index) in enumerate(rs):\n",
      "    svr_poly.fit(train_data.ix[train_index, 'season':], train_labels.ix[train_index, 'count'])\n",
      "    prediction_values = svr_poly.predict(train_data.ix[test_index, 'season':])\n",
      "    print(\"Cross-Validated Error Loop {0}: {1}\".format(i, rmsle(train_labels.ix[test_index, 'count'], prediction_values)))\n",
      "\"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "'\\nprint(\"For the SVR using a linear kernel:\")\\nfor i, (train_index, test_index) in enumerate(rs):\\n    svr_lin.fit(train_data.ix[train_index, \\'season\\':], train_labels.ix[train_index, \\'count\\'])\\n    prediction_values = svr_lin.predict(train_data.ix[test_index, \\'season\\':])\\n    print(\"Cross-Validated Error Loop {0}: {1}\".format(i, rmsle(train_labels.ix[test_index, \\'count\\'], prediction_values)))\\n    \\n# Train the model using the training sets\\nprint(\"For the SVR using an rbf kernel:\")\\nfor i, (train_index, test_index) in enumerate(rs):\\n    svr_rbf.fit(train_data.ix[train_index, \\'season\\':], train_labels.ix[train_index, \\'count\\'])\\n    prediction_values = svr_rbf.predict(train_data.ix[test_index, \\'season\\':])\\n    print(\"Cross-Validated Error Loop {0}: {1}\".format(i, rmsle(train_labels.ix[test_index, \\'count\\'], prediction_values)))\\n    \\n# Train the model using the training sets\\nprint(\"For the SVR using a polynomial kernel or degree 2:\")\\nfor i, (train_index, test_index) in enumerate(rs):\\n    svr_poly.fit(train_data.ix[train_index, \\'season\\':], train_labels.ix[train_index, \\'count\\'])\\n    prediction_values = svr_poly.predict(train_data.ix[test_index, \\'season\\':])\\n    print(\"Cross-Validated Error Loop {0}: {1}\".format(i, rmsle(train_labels.ix[test_index, \\'count\\'], prediction_values)))\\n'"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Decision-Tree Based Approaches"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "RF and GBM extend decision trees....really ensemble methods but we'll cover ensembling multiple models later..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO: Random Forest and GBM Regression\n",
      "# Create rf and gbm regression object\n",
      "rf = RandomForestRegressor(random_state=0, n_estimators=100)\n",
      "gbm = GradientBoostingRegressor(loss='ls', alpha=0.95, n_estimators=500, max_depth=4, learning_rate=.01, min_samples_leaf=9, min_samples_split=9)\n",
      "\n",
      "# Train the model using the training sets\n",
      "print(\"For the Random Forest model:\")\n",
      "for i, (train_index, test_index) in enumerate(rs):\n",
      "    rf.fit(train_data.ix[train_index, 'season':], train_labels.ix[train_index, 'count'])\n",
      "    prediction_values = rf.predict(train_data.ix[test_index, 'season':])\n",
      "    print(\"Cross-Validated Error Loop {0}: {1}\".format(i, rmsle(train_labels.ix[test_index, 'count'], prediction_values)))\n",
      "    \n",
      "# Train the model using the training sets\n",
      "print(\"\\nFor the Gradient Boosted Trees model:\")\n",
      "for i, (train_index, test_index) in enumerate(rs):\n",
      "    gbm.fit(train_data.ix[train_index, 'season':], train_labels.ix[train_index, 'count'])\n",
      "    prediction_values = gbm.predict(train_data.ix[test_index, 'season':])\n",
      "    print(\"Cross-Validated Error Loop {0}: {1}\".format(i, rmsle(train_labels.ix[test_index, 'count'], prediction_values)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "For the Random Forest model:\n",
        "Cross-Validated Error Loop 0: 0.421514143521"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Cross-Validated Error Loop 1: 0.396329978817"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Cross-Validated Error Loop 2: 0.402296540152"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Cross-Validated Error Loop 3: 0.426064097921"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Cross-Validated Error Loop 4: 0.400283032897"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Cross-Validated Error Loop 5: 0.406333535205"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Cross-Validated Error Loop 6: 0.412426432577"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Cross-Validated Error Loop 7: 0.41586217763"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Cross-Validated Error Loop 8: 0.403211228176"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Cross-Validated Error Loop 9: 0.394609981797"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "For the Gradient Boosted Trees model:\n",
        "Cross-Validated Error Loop 0: 0.60607611523"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Cross-Validated Error Loop 1: 0.596993930726"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Cross-Validated Error Loop 2: 0.603294938186"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Cross-Validated Error Loop 3: 0.627104035823"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Cross-Validated Error Loop 4: 0.577069451359"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Cross-Validated Error Loop 5: 0.582865888929"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Cross-Validated Error Loop 6: 0.622736013451"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Cross-Validated Error Loop 7: 0.614366274633"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Cross-Validated Error Loop 8: 0.574499432299"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Cross-Validated Error Loop 9: 0.599122687943"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Feature Engineering"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can start playing with the features..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO: Add Sliding Windows"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO: Add some techniques from signal processing"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Feature Selection"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we've built more features, we need to do some feature selection...ADD MORE"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Modelling: Round Two!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Time to try some of these models again with the advanced features...this time we'll optimize the parameters also...ADD MORE"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Ensembling!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have our models all tuned, we can ensemble them together for superior performance...ADD MORE"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Submitting"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For kaggle, we can simply build a csv using the sample submission"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# For now train using the RF model on the entire training set\n",
      "rf = RandomForestRegressor(random_state=0)\n",
      "\n",
      "# Set the parameters by cross-validation\n",
      "param_grid = {'n_estimators': [100, 300, 500, 700, 1000], 'max_depth': [None, 1, 2, 3, 5], 'min_samples_split': [1, 2, 3, 5]}\n",
      "model = GridSearchCV(rf, param_grid=param_grid)\n",
      "model.fit(train_data.ix[:, 'season':], train_labels.ix[:, 'count'])\n",
      "\n",
      "# Make predictions\n",
      "prediction_values = model.predict(prediction_data.ix[:, 'season':])\n",
      "\n",
      "# Create submission from sample_submission file\n",
      "submission_df = pd.read_csv('./data/output/sampleSubmission.csv')\n",
      "submission_df['count'] = prediction_values\n",
      "submission_df.to_csv('./data/output/rf_tuned_simplefeatures.csv', index=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What have we learned?..."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "APPENDIX I: Deployment"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since this was a kaggle project, we could just prep the .csv. If we're deploying for an app, the default way to do that is to serialize the model and then set up a simple REST service. Typically you don't need real online learning and can get away with retraining the model. That means we just need to build a service that takes an input, performs the feature transformations to shape our date into the same shape as the engineered features, and load the serialized model to make a prediction"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO: Serialize and save"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO: Server"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are all sorts of different ways to deploy. If we use simpler models, we can serialize to more portable PMML like @ AirBnB. That we can use the decision boundaries in any other language...ADD MORE"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "APPENDIX II: Deep Learning"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So using supervised learning, the sliding window approaches proved to be instrumental and feature engineering is obviously super important. However, with the modern advances in ML over the last few years, we can achieve the same (or superior) performance without domain-specific features. See LINK for tutorial, but ultimately the importance of Deep Learning is representation learning...ADD MORE"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO: RBM, Stacked Auto-encoders, DBN, CNN and RNN?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}